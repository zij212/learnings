{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalized Linear Model with Gradient Descent\n",
    "\n",
    "1. [gradient descent](#gradient-descent)\n",
    "1. [implement gradient descent for linear regression from scratch](#implement-gradient-descent-for-linear-regression-from-scratch)\n",
    "1. [implement gradient descent for logistic regression from scratch](#implement-gradient-descent-for-logistic-regression-from-scratch)\n",
    "1. [resources](#resources)\n",
    "\n",
    "## Gradient Descent\n",
    "\n",
    "The OLS notebook discussed the closed form solution of linear regression \n",
    "\n",
    "\\begin{align}\n",
    "\\min_{\\beta} L & = \\mathbf{(y-X\\beta)^T(y-X\\beta)} \\\\ \n",
    "\\frac{dL}{d\\mathbf{\\beta}} & = \\mathbf{-2X^T(y-X\\beta)} = 0\\\\\n",
    "& \\mathbf{ \\hat{\\beta}=(X^TX)^{-1}X^Ty}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "When the dataset is large, it is not practical to find the closed form solution as the computation complexity of inverting an $n * n$ matrix ($X$ has $n$ observations) is $O(n^3)$ <sup>[1]</sup>.\n",
    "\n",
    "Instead, we iteratively update the weight using \n",
    "\n",
    "\\begin{align}\n",
    "\\beta & \\rightarrow \\beta - \\eta \\nabla_{\\beta}L(\\beta) \\\\\n",
    "\\beta & \\rightarrow \\beta - \\eta \\mathbf{(-2X^T(y-X\\beta))} \n",
    "\\end{align}\n",
    "\n",
    "where $\\eta$ is the learning rate, and $\\nabla_{\\beta}L(\\beta)$ is the gradient vector contains the partial derivatices of the cost function with respect to each model parameter.\n",
    "\n",
    "There are 3 types of gradient descent algorithms,\n",
    "- batch gradient descent: at each updating step, the entire trainig set is used to calculate the gradient vector.\n",
    "- stochastic gradient descent: at each updating step, train on a single random observation.\n",
    "- mini batch gradient descent: at each updating step, train on a small random sets of observations.\n",
    "\n",
    "In the next section, we'll implement gradient descent from scratch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Gradient Descent for Linear Regression from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class LinearRegressionMBGD:\n",
    "    def __init__(self, batch_size: int=32, lr: float=0.01, max_iters: int=100):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            batch_size (int, optional): number of observations to train on at each step. Defaults to 32.\n",
    "            lr (float, optional): learning rate. Defaults to 0.01.\n",
    "            max_iters (float, optional): maximum number of training steps. Defaults to 100.\n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.max_iters = max_iters\n",
    "    \n",
    "    def gradient(self, X, y, beta):\n",
    "        return - 2 * X.T @ (y - X @ beta)\n",
    "    \n",
    "    \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        # initialize the weight with 0s\n",
    "        n = X.shape[1]\n",
    "        beta = np.zeros(n)\n",
    "\n",
    "        for _ in range(self.max_iters):\n",
    "            # randomly select a batch of observations\n",
    "            idx = np.random.choice(len(y), self.batch_size, replace=False)\n",
    "            X_mini_batch, y_mini_batch = X[idx, :], y[idx]\n",
    "            # update to \\beta - \\eta * \\nabla_{\\beta} L(\\beta)\n",
    "            beta -= self.lr * self.gradient(X_mini_batch, y_mini_batch, beta)\n",
    "\n",
    "        self.beta = beta\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        return X @ self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True parameter: [-0.00230347  1.02378018 -0.32812042 -0.49768963]\n"
     ]
    }
   ],
   "source": [
    "n = 4\n",
    "m = 1000 \n",
    "X = np.random.normal(size=(m, n))\n",
    "b_true = np.random.normal(size=n)\n",
    "e = np.random.normal(size=m)\n",
    "y = X @ b_true + e\n",
    "print(f\"True parameter: {b_true}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_hat (mbgd):      [ 0.01535948  0.97083694 -0.3764715  -0.55437832]\n",
      "beta_hat (analytical):[-0.01534097  1.00746112 -0.33289216 -0.55708937]\n"
     ]
    }
   ],
   "source": [
    "lr_mbgd = LinearRegressionMBGD(lr=0.001, max_iters=2000)\n",
    "lr_mbgd.fit(X, y)\n",
    "\n",
    "print(f\"{'beta_hat (mbgd):':<21} {lr_mbgd.beta}\")\n",
    "print(f\"{'beta_hat (analytical):':<21}{np.linalg.inv(X.T @ X) @ X.T @ y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Gradient Descent for Logistic Regression from Scratch\n",
    "\n",
    "Logistic regression can be written as\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{s} & = \\mathbf{X\\beta} \\\\\n",
    "\\mathbf{y} & = \\mathbf{\\sigma(s)} \\\\\n",
    "\\end{align}\n",
    "\n",
    "where $\\mathbf{\\sigma(\\cdot)}$ is the [sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function) or the [error function](https://en.wikipedia.org/wiki/Error_function). \n",
    "\n",
    "In this section we'll use the sigmoid function $\\sigma(x)=\\frac{1}{1+e^{-x}}$ which has a intersting property that $\\frac{d}{dx}\\sigma(x)=\\sigma(x)(1-\\sigma(x))$\n",
    "\n",
    "Maximizing the log likelihood of observing $\\mathbf{y}$ given the feature set $\\mathbf{X}$ and modeling hypothesis $\\mathbf{h}$ \n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{1_n^T \\log prob(y | X; h)}\n",
    "\\end{align}\n",
    "\n",
    "is equivalent to minimizing the binary cross entropy loss <sup>[2]</sup>\n",
    "\n",
    "\\begin{align}\n",
    "L = \\mathbf{-y^T \\log p - (1_n - y)^T \\log (1_n - p)} \\\\\n",
    "\\mathbf{p = \\sigma(\\hat{s}) = \\sigma(X\\hat{\\beta}) = \\frac{1}{1+e^{-X\\hat{\\beta}}}} \n",
    "\\end{align}\n",
    "\n",
    "To get the gradient of the loss function with respect to $\\beta$, we need to apply the chain rule\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{dL}{d\\beta} & = \\frac{dL}{d\\mathbf{p}} \\frac{d\\mathbf{p}}{d\\mathbf{s}} \\frac{d\\mathbf{s}}{d\\beta} \\\\\n",
    "\\frac{dL}{d\\mathbf{p}} & = \\mathbf{-y \\oslash p - (-1)* (1_n - y) \\oslash (1_n - p)}\\\\\n",
    "\\frac{d\\mathbf{p}}{d\\mathbf{s}} & = \\mathbf{p \\odot (1_n - p)}\\\\\n",
    "\\frac{d\\mathbf{s}}{d\\beta} & = \\mathbf{X} \\\\\n",
    "\\frac{dL}{d\\beta}  = \\mathbf{X^T (-y \\odot (1_n - p) + (1_n - y) \\odot p)} & = \\mathbf{X^T (p-y)} \n",
    "\\end{align}\n",
    "\n",
    "$\\oslash$ is element-wise division and $\\odot$ is element-wise product\n",
    "\n",
    "\n",
    "The update rule can be simplified to\n",
    "\n",
    "\\begin{align}\n",
    "\\beta & \\rightarrow \\beta - \\eta \\nabla_{\\beta} L \\\\\n",
    "\\beta & \\rightarrow \\beta - \\eta\\mathbf{X^T (p-y)}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def sigmoid(x: Union[int, float, np.ndarray]) -> Union[float, np.ndarray]:\n",
    "    # cliping at the extremes of 64bit floating point numbers\n",
    "    x = np.clip(x, -709.78, 709.78)\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def cross_entropy_loss(y: np.ndarray, p: np.ndarray) -> float:\n",
    "    \"\"\"average cross entropy loss\n",
    "\n",
    "    Args:\n",
    "        y (np.ndarray): label\n",
    "        p (np.ndarray): predicted probability\n",
    "\n",
    "    Returns:\n",
    "        float: _description_\n",
    "    \"\"\"\n",
    "    l = - y.T @ np.log(p) - (1 - y).T @ np.log(1 - p)\n",
    "    n = len(y)\n",
    "    return l / n\n",
    "\n",
    "\n",
    "class LogisticRegressionGD:\n",
    "    def __init__(self, batch_size: int=32, lr: float=0.01, max_iters: int=100):\n",
    "        \"\"\"Logistic regression using gradient descent\n",
    "\n",
    "        Args:\n",
    "            batch_size (int, optional): number of observations to train on at each step. Defaults to 32.\n",
    "            lr (float, optional): learning rate. Defaults to 0.01.\n",
    "            max_iters (float, optional): maximum number of training steps. Defaults to 100.\n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.max_iters = max_iters\n",
    "    \n",
    "    def gradient(self, X, y, beta):\n",
    "        p = sigmoid(X @ beta)\n",
    "        return X.T @ (p - y)\n",
    "    \n",
    "    \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        # initialize the weight with 0s\n",
    "        n = X.shape[1]\n",
    "        beta = np.zeros(n)\n",
    "\n",
    "        self.losses = []\n",
    "\n",
    "        for _ in range(self.max_iters):\n",
    "            # randomly select a batch of observations\n",
    "            idx = np.random.choice(len(y), self.batch_size, replace=False)\n",
    "            X_mini_batch, y_mini_batch = X[idx, :], y[idx]\n",
    "            # update to \\beta - \\eta * \\nabla_{\\beta} L(\\beta)\n",
    "            beta -= self.lr * self.gradient(X_mini_batch, y_mini_batch, beta)\n",
    "            # record the cross entropy loss\n",
    "            p_mini_batch = self._predict_proba(X_mini_batch, beta)\n",
    "            self.losses.append(cross_entropy_loss(y_mini_batch, p_mini_batch))\n",
    "\n",
    "        self.beta = beta\n",
    "\n",
    "    def _predict_proba(self, X, beta, eps=1e-15):\n",
    "        # cliping to avoid run time warning for np.log(0)\n",
    "        return np.clip(sigmoid(X @ beta), eps, 1-eps)\n",
    "    \n",
    "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        return self._predict_proba(X, self.beta, eps=0)\n",
    "    \n",
    "    def predict(self, X: np.ndarray, threshold=0.5) -> np.ndarray:\n",
    "        return (self.predict_proba(X) > threshold).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to run some data through the above implementation. We'll use the breast cancer binary classification dataset from sklearn, and predict the diagnosis using mean radius, mean texture, and mean symmetry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])\n",
      "['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
      " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
      " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
      " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
      " 'smoothness error' 'compactness error' 'concavity error'\n",
      " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
      " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
      " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
      " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n",
      "['malignant' 'benign']\n",
      "(array([0, 1]), array([212, 357]))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "\n",
    "data = load_breast_cancer()\n",
    "print(data.keys())\n",
    "\n",
    "X = data.data\n",
    "y = data.target\n",
    "print(data.feature_names)\n",
    "print(data.target_names)\n",
    "print(np.unique(y, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17.99   10.38    0.2419]\n",
      " [20.57   17.77    0.1812]\n",
      " [19.69   21.25    0.2069]]\n",
      "[[1.         0.32378189 0.14774124]\n",
      " [0.32378189 1.         0.07140098]\n",
      " [0.14774124 0.07140098 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "feature_names = [\"mean radius\", \"mean texture\", \"mean symmetry\"]\n",
    "mask = np.isin(data.feature_names, feature_names)\n",
    "feature_idx = np.argwhere(mask).ravel()\n",
    "X = X[:, feature_idx]\n",
    "\n",
    "print(X[:3])\n",
    "print(np.corrcoef(X.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at `X[:3]`, it's clear that the features are in different scales. \n",
    "\n",
    "Since the same learning rate is applied to all model parameters in our algorithm, unscaled features will make the updates unstable. In this case we should standardize the feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.09706398e+00, -2.07333501e+00,  2.21751501e+00],\n",
       "       [ 1.82982061e+00, -3.53632408e-01,  1.39236330e-03],\n",
       "       [ 1.57988811e+00,  4.56186952e-01,  9.39684817e-01]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "X = StandardScaler().fit_transform(X)\n",
    "X[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.31417424, -1.12064318, -1.36766058])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mini-batch gradient descent: 1 < batch_size << len(y) \n",
    "# stochastic gradient descent: batch_size = 1 \n",
    "# batch gradient descent: batch_size = len(y)\n",
    "\n",
    "m = LogisticRegressionGD(lr=0.01, batch_size=32, max_iters=1000)\n",
    "m.fit(X_train, y_train)\n",
    "m.beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training data through scikit-learn and statsmodels implementation of logistic regression and compare the result on the testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.225719\n",
      "         Iterations 9\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# disable regularization and intercept to match sm.Logit() and LogisticRegressionGD()\n",
    "m_sl = LogisticRegression(max_iter=1000, fit_intercept=False, penalty=None)\n",
    "m_sl.fit(X_train, y_train)\n",
    "b_sl = np.hstack(m_sl.coef_)\n",
    "\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "m_sm = sm.Logit(y_train, X_train).fit()\n",
    "b_sm = m_sm.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mbgd       roc_auc: 0.94, precision: 0.96, recall: 0.94, weights[-4.31417424 -1.12064318 -1.36766058]\n",
      "sklearn    roc_auc: 0.94, precision: 0.96, recall: 0.96, weights[-4.34156853 -1.11208314 -1.42974143]\n",
      "statsmodel roc_auc: 0.94, precision: 0.96, recall: 0.96, weights[-4.34539123 -1.11329859 -1.43053815]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "for model_name, y_pred, b in zip(\n",
    "    [\"mbgd\", \"sklearn\", \"statsmodel\"], \n",
    "    [m.predict(X_test), m_sl.predict(X_test), (m_sm.predict(X_test) > 0.5).astype(int)],\n",
    "    [m.beta, b_sl, b_sm]\n",
    "    ):\n",
    "    roc_auc = roc_auc_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    print(f\"{model_name:<10} roc_auc: {roc_auc:.2f}, precision: {precision:.2f}, recall: {recall:.2f}, weights{b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "variable=loss<br>iter=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "loss",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "loss",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499,
          500,
          501,
          502,
          503,
          504,
          505,
          506,
          507,
          508,
          509,
          510,
          511,
          512,
          513,
          514,
          515,
          516,
          517,
          518,
          519,
          520,
          521,
          522,
          523,
          524,
          525,
          526,
          527,
          528,
          529,
          530,
          531,
          532,
          533,
          534,
          535,
          536,
          537,
          538,
          539,
          540,
          541,
          542,
          543,
          544,
          545,
          546,
          547,
          548,
          549,
          550,
          551,
          552,
          553,
          554,
          555,
          556,
          557,
          558,
          559,
          560,
          561,
          562,
          563,
          564,
          565,
          566,
          567,
          568,
          569,
          570,
          571,
          572,
          573,
          574,
          575,
          576,
          577,
          578,
          579,
          580,
          581,
          582,
          583,
          584,
          585,
          586,
          587,
          588,
          589,
          590,
          591,
          592,
          593,
          594,
          595,
          596,
          597,
          598,
          599,
          600,
          601,
          602,
          603,
          604,
          605,
          606,
          607,
          608,
          609,
          610,
          611,
          612,
          613,
          614,
          615,
          616,
          617,
          618,
          619,
          620,
          621,
          622,
          623,
          624,
          625,
          626,
          627,
          628,
          629,
          630,
          631,
          632,
          633,
          634,
          635,
          636,
          637,
          638,
          639,
          640,
          641,
          642,
          643,
          644,
          645,
          646,
          647,
          648,
          649,
          650,
          651,
          652,
          653,
          654,
          655,
          656,
          657,
          658,
          659,
          660,
          661,
          662,
          663,
          664,
          665,
          666,
          667,
          668,
          669,
          670,
          671,
          672,
          673,
          674,
          675,
          676,
          677,
          678,
          679,
          680,
          681,
          682,
          683,
          684,
          685,
          686,
          687,
          688,
          689,
          690,
          691,
          692,
          693,
          694,
          695,
          696,
          697,
          698,
          699,
          700,
          701,
          702,
          703,
          704,
          705,
          706,
          707,
          708,
          709,
          710,
          711,
          712,
          713,
          714,
          715,
          716,
          717,
          718,
          719,
          720,
          721,
          722,
          723,
          724,
          725,
          726,
          727,
          728,
          729,
          730,
          731,
          732,
          733,
          734,
          735,
          736,
          737,
          738,
          739,
          740,
          741,
          742,
          743,
          744,
          745,
          746,
          747,
          748,
          749,
          750,
          751,
          752,
          753,
          754,
          755,
          756,
          757,
          758,
          759,
          760,
          761,
          762,
          763,
          764,
          765,
          766,
          767,
          768,
          769,
          770,
          771,
          772,
          773,
          774,
          775,
          776,
          777,
          778,
          779,
          780,
          781,
          782,
          783,
          784,
          785,
          786,
          787,
          788,
          789,
          790,
          791,
          792,
          793,
          794,
          795,
          796,
          797,
          798,
          799,
          800,
          801,
          802,
          803,
          804,
          805,
          806,
          807,
          808,
          809,
          810,
          811,
          812,
          813,
          814,
          815,
          816,
          817,
          818,
          819,
          820,
          821,
          822,
          823,
          824,
          825,
          826,
          827,
          828,
          829,
          830,
          831,
          832,
          833,
          834,
          835,
          836,
          837,
          838,
          839,
          840,
          841,
          842,
          843,
          844,
          845,
          846,
          847,
          848,
          849,
          850,
          851,
          852,
          853,
          854,
          855,
          856,
          857,
          858,
          859,
          860,
          861,
          862,
          863,
          864,
          865,
          866,
          867,
          868,
          869,
          870,
          871,
          872,
          873,
          874,
          875,
          876,
          877,
          878,
          879,
          880,
          881,
          882,
          883,
          884,
          885,
          886,
          887,
          888,
          889,
          890,
          891,
          892,
          893,
          894,
          895,
          896,
          897,
          898,
          899,
          900,
          901,
          902,
          903,
          904,
          905,
          906,
          907,
          908,
          909,
          910,
          911,
          912,
          913,
          914,
          915,
          916,
          917,
          918,
          919,
          920,
          921,
          922,
          923,
          924,
          925,
          926,
          927,
          928,
          929,
          930,
          931,
          932,
          933,
          934,
          935,
          936,
          937,
          938,
          939,
          940,
          941,
          942,
          943,
          944,
          945,
          946,
          947,
          948,
          949,
          950,
          951,
          952,
          953,
          954,
          955,
          956,
          957,
          958,
          959,
          960,
          961,
          962,
          963,
          964,
          965,
          966,
          967,
          968,
          969,
          970,
          971,
          972,
          973,
          974,
          975,
          976,
          977,
          978,
          979,
          980,
          981,
          982,
          983,
          984,
          985,
          986,
          987,
          988,
          989,
          990,
          991,
          992,
          993,
          994,
          995,
          996,
          997,
          998,
          999
         ],
         "xaxis": "x",
         "y": [
          0.6387895698588183,
          0.5721500713795403,
          0.5215178822895836,
          0.5165010441513452,
          0.48102465578281545,
          0.45952973122289065,
          0.4205077272869311,
          0.42869330323805976,
          0.4351033631235906,
          0.41927525528238146,
          0.38059398449468984,
          0.34482339723844585,
          0.36009124232925377,
          0.3949281441459921,
          0.3333346583336774,
          0.3073359569422983,
          0.3409742028626642,
          0.43803353312350624,
          0.26531729732799275,
          0.37958210453655583,
          0.317057426799646,
          0.2878808743351523,
          0.38723506323449586,
          0.364190748992167,
          0.3751165143686807,
          0.2936452874939303,
          0.3497560658835915,
          0.2650260387497281,
          0.3263989764013069,
          0.3282631588007836,
          0.3909967244893233,
          0.23571579359569833,
          0.3215385693732968,
          0.31191075306341864,
          0.31025863553076594,
          0.2780156903788995,
          0.21731872541090935,
          0.38585371180413675,
          0.2563680286305499,
          0.26753019847476345,
          0.21505048874502697,
          0.2477501450183184,
          0.332379486771954,
          0.3969569221601428,
          0.2373630822137148,
          0.2242997361153111,
          0.26762730673437707,
          0.2705260628025846,
          0.24656888071165467,
          0.26257579594690134,
          0.2894768597637709,
          0.3431836760255496,
          0.242530035214217,
          0.4031167517759148,
          0.31487671396787703,
          0.41401399554612506,
          0.28086557397126205,
          0.39490167746654614,
          0.34342185863369673,
          0.375116037201145,
          0.2953086960661806,
          0.22979311451995943,
          0.4111115478653893,
          0.23381587692782718,
          0.18831036829855743,
          0.3161660643933763,
          0.1706746886702235,
          0.2193247890968199,
          0.20999260223746638,
          0.2610868458773711,
          0.34265109357387935,
          0.3681563340076287,
          0.2911549266971054,
          0.2588169918675543,
          0.1962692893219503,
          0.310149349191718,
          0.26036048896946706,
          0.2133678315707036,
          0.3849648402669474,
          0.26729767400728666,
          0.3116975937529649,
          0.2674010190193467,
          0.2221907071379644,
          0.24643096776031137,
          0.1725128984660767,
          0.28353035226134227,
          0.22665098160686736,
          0.2580134456025053,
          0.2807962779641119,
          0.25525615603076773,
          0.18896028336152676,
          0.3042282844402578,
          0.3625417040120072,
          0.14886513634064155,
          0.2113690000304167,
          0.3365041174031216,
          0.2545466952438119,
          0.15274334318874216,
          0.2632486950972234,
          0.16430235087503897,
          0.27315197737064256,
          0.1719140396801349,
          0.26149717928604405,
          0.2430145912561391,
          0.25821326593371685,
          0.10881638390861365,
          0.2259002186056914,
          0.2827102838583049,
          0.2547386508094678,
          0.2668924692233384,
          0.4019841233795349,
          0.17192999786084623,
          0.27382565533325887,
          0.3613604362621463,
          0.3085622816817363,
          0.15907047391463655,
          0.26673776480777023,
          0.3078036646167591,
          0.38175170523404345,
          0.233839016612869,
          0.26125023607875103,
          0.2670838012957894,
          0.24590102024631832,
          0.13742785146216557,
          0.26685530491516235,
          0.17167481557691588,
          0.3543486625179489,
          0.11834269110913513,
          0.2220813744267333,
          0.15967426848453203,
          0.22155112715919223,
          0.20404076639084795,
          0.25124344995647074,
          0.2663426087598371,
          0.24756798940078584,
          0.2574178760275898,
          0.2569580740242406,
          0.17857853742552737,
          0.16508596466915346,
          0.21250172926294308,
          0.22063890432642821,
          0.3035353468351162,
          0.23520199917027207,
          0.2204587517995704,
          0.1879046910726932,
          0.23413917950401053,
          0.306979334442891,
          0.27563393434449796,
          0.13345873518460208,
          0.19308918787128967,
          0.3526905921997149,
          0.29925630875826287,
          0.17615835995933105,
          0.5212482880400379,
          0.2065215598081488,
          0.22458947708532906,
          0.1342941489395243,
          0.1448347629701549,
          0.3783137228688185,
          0.24912164133490833,
          0.20036073979512403,
          0.2684054116321153,
          0.2426517936163835,
          0.1953079729325835,
          0.3473590730170841,
          0.26281828408872976,
          0.13944706382816957,
          0.25275608204111116,
          0.12832487845577079,
          0.3666978286429395,
          0.21852275723729214,
          0.22176285252455058,
          0.13193499102111478,
          0.27995037619383617,
          0.24905399042225784,
          0.1405748640929425,
          0.24177527167545668,
          0.1563522787857279,
          0.3592808090676478,
          0.24666610775895817,
          0.2509897584614329,
          0.22116777049327002,
          0.2499344903159811,
          0.2766566169584225,
          0.1530440894062018,
          0.2467864218640433,
          0.12794199382976568,
          0.25991075190311563,
          0.17369990976955185,
          0.359026623943403,
          0.17482864100087714,
          0.2531569831119387,
          0.16886953566604063,
          0.28087204887198025,
          0.27556148759491816,
          0.32541079112596083,
          0.24226353951675764,
          0.2304713409214753,
          0.1723470066622681,
          0.11824847126449471,
          0.2998335078521759,
          0.23885259960268523,
          0.27352552451243406,
          0.23888606053741004,
          0.253958643008931,
          0.20539073479299091,
          0.1974375539564741,
          0.23367728793689632,
          0.17674994743285427,
          0.35235480226184046,
          0.27917426062901207,
          0.2711814879514437,
          0.30454566765733015,
          0.2997887283590388,
          0.08925835885524296,
          0.1914475204087107,
          0.21691140625400268,
          0.1970229636554618,
          0.17270774664913147,
          0.2194244465967113,
          0.2393343275947723,
          0.13890488049990835,
          0.3151510099127282,
          0.28420586205665055,
          0.1797921376262385,
          0.3498668543724123,
          0.2499810946658652,
          0.1923401827391082,
          0.27688400081375675,
          0.18006584752464658,
          0.20117818403341878,
          0.09313224040938473,
          0.3188012518446798,
          0.29193266807139745,
          0.24559620580924732,
          0.15294065391289652,
          0.25350605722122693,
          0.22926845889304004,
          0.19735457490183445,
          0.22832159705989805,
          0.28695739261463515,
          0.09917277492885859,
          0.17840748174031973,
          0.3892927084810241,
          0.2779319839763693,
          0.2924924577465421,
          0.25737928965205037,
          0.24466039130065914,
          0.24072060515876664,
          0.1597040373803854,
          0.17221702204107037,
          0.3211622239189297,
          0.18980471681879038,
          0.32029541132819,
          0.36826488053199696,
          0.1972873456281441,
          0.2450551947083674,
          0.3121045638330064,
          0.15320297187545295,
          0.08383420191671634,
          0.16790345018783945,
          0.07799584337325711,
          0.16227488918949975,
          0.31716980984943954,
          0.11637011026734453,
          0.1597621709920379,
          0.19835847442168295,
          0.286667700824653,
          0.19590359765177268,
          0.19676625051566649,
          0.4009554497355706,
          0.2471575428581125,
          0.1605892258524732,
          0.17362530301323695,
          0.22752304577326565,
          0.1641616918868042,
          0.1409981961489957,
          0.32600243326002815,
          0.28614963612362343,
          0.3125704541098324,
          0.27378913013216466,
          0.22271808297906853,
          0.19557882323290354,
          0.1548313103164033,
          0.09798266431128236,
          0.17856606004495384,
          0.23784559808731187,
          0.13542666200148729,
          0.14313443480223526,
          0.2661158253951154,
          0.08971286557326112,
          0.15332504918478057,
          0.245966676684958,
          0.2558836015729322,
          0.15179292242893905,
          0.20530841940204203,
          0.19394061812512225,
          0.17244336774135438,
          0.347822954775128,
          0.311428144072611,
          0.25275061596533277,
          0.20415854388722401,
          0.14920200496362052,
          0.1407579039565574,
          0.19426423397258108,
          0.19370564796962608,
          0.3777471840454849,
          0.2944014484602751,
          0.27783984130707384,
          0.15621989202495334,
          0.1351473127427103,
          0.2516902329544253,
          0.20420278679420123,
          0.3809903087154418,
          0.4065354744231179,
          0.11173056315412525,
          0.24818450691350427,
          0.12111123303071228,
          0.2599442905619875,
          0.21166915431691194,
          0.26715807174535977,
          0.2057870954056782,
          0.1549694115323184,
          0.36320561299259363,
          0.23451982373085806,
          0.1946497522756448,
          0.2632137011180243,
          0.14713808961654243,
          0.20578567763376548,
          0.24276979258157755,
          0.18137368884802263,
          0.27799905507713907,
          0.24481208572321944,
          0.20755580878452384,
          0.2117668791654278,
          0.3196472347866538,
          0.14986946054615288,
          0.357956273467374,
          0.2860349032461606,
          0.17788575190104428,
          0.24383414367405085,
          0.21312661387371906,
          0.22945053163291668,
          0.18189029028722598,
          0.16478790298702958,
          0.1726899739624775,
          0.2731745355264898,
          0.20538859500293025,
          0.15864249979452694,
          0.2805325677845039,
          0.22443450642706556,
          0.17138939034231618,
          0.34143509062601307,
          0.06367877549907858,
          0.16829836887088948,
          0.2931827408347673,
          0.20083506308743432,
          0.12863632574020611,
          0.2096849199072131,
          0.13345116847446795,
          0.15503732879162072,
          0.26704231942091006,
          0.4019730815541943,
          0.29606983152761623,
          0.19636212105923032,
          0.14672796755187573,
          0.2589769023124363,
          0.3419502868081682,
          0.21218453603898219,
          0.1915138954148034,
          0.21470031476658413,
          0.2700374414122059,
          0.37394981400345567,
          0.13488906454896302,
          0.21650105929452518,
          0.24399051312778397,
          0.2602748477484537,
          0.24836753943174034,
          0.11404908063908459,
          0.19041970118376267,
          0.35360901441052583,
          0.240167380587759,
          0.3179161179888654,
          0.13890258190984434,
          0.3108554654100817,
          0.2535424271762029,
          0.17624214072811575,
          0.26425482608471157,
          0.23773504883460198,
          0.3140554929304197,
          0.24253828077349565,
          0.23383802799387426,
          0.23789581696472184,
          0.2980484458974595,
          0.15133791561775445,
          0.11475305167167832,
          0.2048899777471058,
          0.061390504873288586,
          0.13655761056048965,
          0.11904219281601011,
          0.15863625118404673,
          0.20669409522072374,
          0.2773520997024393,
          0.15975864060859712,
          0.24986017627732307,
          0.36254071732862203,
          0.252710854474392,
          0.297563384028379,
          0.25760966588606343,
          0.17887720159688136,
          0.18057791288041525,
          0.27787317375298154,
          0.16542454940171258,
          0.18670613707182157,
          0.38020643272803434,
          0.33787502556904353,
          0.22422805390692818,
          0.2329602686136702,
          0.24921854641756205,
          0.3031283633238036,
          0.2835912776939808,
          0.29402140760045054,
          0.15940508449613938,
          0.25024208980396373,
          0.28275590024200586,
          0.3005632801327849,
          0.44117780371724585,
          0.15692035874417637,
          0.15982026218051526,
          0.17355027297479103,
          0.2617689794840072,
          0.3306600636343169,
          0.21252196896718256,
          0.20247674297304707,
          0.34065102035914174,
          0.2629430287463942,
          0.19386052850097,
          0.16727757795290923,
          0.2650673371203768,
          0.16433606813113721,
          0.3498948935256953,
          0.31553101629684155,
          0.14856828980211478,
          0.1971902084933923,
          0.22431322535069342,
          0.21397682577400023,
          0.29220089045223263,
          0.24114398787038827,
          0.21679756476594192,
          0.23039164364487313,
          0.202448237645418,
          0.3316646037020886,
          0.2687371123829526,
          0.14246399695981923,
          0.15034661195693205,
          0.1150916577591922,
          0.19229137188980777,
          0.19368587558536385,
          0.12252753029562127,
          0.17448966240345984,
          0.1847168109740837,
          0.2712526686279061,
          0.3039865750241152,
          0.09369398459727193,
          0.33639358749010523,
          0.11408020734099716,
          0.11923943722921634,
          0.12086897040501228,
          0.16603334210224135,
          0.1533009112786008,
          0.21685911588827497,
          0.17146010500954884,
          0.16358302749364012,
          0.1942727203130399,
          0.18853810911979754,
          0.19733949902649106,
          0.20638726456562456,
          0.1737939502647733,
          0.3647255995607443,
          0.2032332980128368,
          0.2098541991898471,
          0.10697147445463628,
          0.0976107098043367,
          0.2684857576440539,
          0.24044283705142214,
          0.12804998582807595,
          0.15144884160304387,
          0.20036075820757066,
          0.1546306411676527,
          0.18287881586033145,
          0.27889124402201326,
          0.31983399646873933,
          0.2668812920616093,
          0.13570479016257253,
          0.23635274951208193,
          0.2454281311203036,
          0.26167464766361515,
          0.1507349467980336,
          0.17880711473439748,
          0.2425464976094802,
          0.3216459513359764,
          0.23254416011957407,
          0.14949966607898751,
          0.2013833871841821,
          0.1390953202647546,
          0.14391618521240712,
          0.1831977205559675,
          0.2621577360329555,
          0.17182096633208266,
          0.31411135046398553,
          0.2583707427358766,
          0.31126107951837223,
          0.19441782213610034,
          0.1048610411591254,
          0.29033474276392557,
          0.16913334679312128,
          0.3064786748786668,
          0.11081584434915481,
          0.11688795138652927,
          0.3326055314978651,
          0.22873692933628875,
          0.2417651809731725,
          0.3068498484037035,
          0.307932476274126,
          0.21476817436928147,
          0.2744728131920079,
          0.3654223759588189,
          0.13320609614789805,
          0.24984947802958787,
          0.2972495030957791,
          0.2523218200089436,
          0.2259328838076614,
          0.20029362071857607,
          0.22091205545012402,
          0.17004574016487237,
          0.15876164734731718,
          0.42704765354722185,
          0.17072434015210475,
          0.28707349560840734,
          0.11457555385944923,
          0.3224717745331327,
          0.23215153584116088,
          0.16398778611266834,
          0.23486122883307203,
          0.19091364754804627,
          0.1752951288566202,
          0.2240356357669892,
          0.2460999296745553,
          0.2958071005006091,
          0.24453858426344843,
          0.29209806627421137,
          0.10384826817924588,
          0.21932516377008698,
          0.3019132349358217,
          0.24685007356683664,
          0.2103665842653577,
          0.29911266895783645,
          0.16989246435142855,
          0.2212896404093505,
          0.11221333726580467,
          0.18267763732184705,
          0.16301082724529237,
          0.14470038335902607,
          0.19203553224311093,
          0.2245951675352641,
          0.41697579920838446,
          0.21446033586248608,
          0.3163699456065592,
          0.17439751702059117,
          0.31681641786929005,
          0.23808326057916424,
          0.17479152456458077,
          0.2186370391359707,
          0.1346378678313918,
          0.26547840418615487,
          0.49636136389838115,
          0.29368282263801565,
          0.21162433127853458,
          0.19443216765764995,
          0.23126751343484295,
          0.2311737755460394,
          0.12871396389518855,
          0.1141002694002083,
          0.16499605941701484,
          0.3269136618548072,
          0.23319651136773387,
          0.28725653574988175,
          0.14069873580262948,
          0.11205286563589086,
          0.45297036892986775,
          0.2148834337168841,
          0.2962410747408426,
          0.31592337108855617,
          0.045060602384728206,
          0.32217474169629146,
          0.21337949713798626,
          0.03820298539790923,
          0.17835483628790866,
          0.17413660101051207,
          0.13580164427751307,
          0.2121169224432009,
          0.47084054743867143,
          0.27169995535703556,
          0.21557502034915654,
          0.3186305855026739,
          0.11137675207969147,
          0.1926561189651149,
          0.2802906101434768,
          0.1403415340986519,
          0.4501656897183645,
          0.16611400302997734,
          0.3764075300813984,
          0.2783631485799465,
          0.22272107193527096,
          0.3244000001485625,
          0.26396075140347053,
          0.14808490041186173,
          0.2931982376805834,
          0.11467271178980618,
          0.31537944115861566,
          0.23174262919644903,
          0.25208964486478236,
          0.09146910560819621,
          0.23891562355218154,
          0.16092242502183146,
          0.26446403919630224,
          0.24864557538700566,
          0.2573765890874006,
          0.2925729307705856,
          0.21444845596171808,
          0.2434200665292996,
          0.19991134811136205,
          0.12910010460043286,
          0.2906091655715508,
          0.2834651893672947,
          0.2504509909841928,
          0.19731107806990397,
          0.3265414041051751,
          0.2597261778107111,
          0.277894923477998,
          0.28755897035461114,
          0.2776087133241306,
          0.22034246910069474,
          0.2411682298005443,
          0.2702795823437078,
          0.15687714029689576,
          0.1600048365517944,
          0.37820839150651975,
          0.1994115845239582,
          0.23613823335266443,
          0.2933074324081202,
          0.3093355385197655,
          0.3203503780499384,
          0.2584680490318903,
          0.18745477819632606,
          0.33380792452844393,
          0.1925442844788567,
          0.24983001073466302,
          0.21480907713470643,
          0.1768066293292851,
          0.2171759429121221,
          0.22125524727961557,
          0.22074407170979618,
          0.2675704388662906,
          0.19120793735129835,
          0.285718279591079,
          0.1826560048983284,
          0.2640322683462523,
          0.3634449624976674,
          0.433018693636128,
          0.2403313576433428,
          0.22404862430367006,
          0.21958403841976354,
          0.24201623442432474,
          0.31293752809064,
          0.21055952060698613,
          0.3066303478514794,
          0.1565237322086452,
          0.13769155475778022,
          0.29016357842453,
          0.2988461504840353,
          0.2741072579696525,
          0.08464506173814162,
          0.16412831934077438,
          0.1996559892112736,
          0.25596660396009013,
          0.1027443887427148,
          0.23893996786781557,
          0.3001673733817789,
          0.29624819093499494,
          0.19188387314283795,
          0.4883593601096474,
          0.2700907486776555,
          0.23705515874163322,
          0.19504192939932366,
          0.1242746411891352,
          0.26852944166830767,
          0.18342890238432433,
          0.37937150314135165,
          0.2079776520340903,
          0.45751199132072473,
          0.24479105957243424,
          0.26964507041970376,
          0.1905342832337831,
          0.3459576328924589,
          0.20173667812336302,
          0.29663703964945,
          0.22209350597468516,
          0.17243360058759452,
          0.29792828211001954,
          0.32684871757974077,
          0.19447323516166512,
          0.2615151762456909,
          0.17405889246974068,
          0.2029019160117318,
          0.24384824598770183,
          0.14791411861706233,
          0.35247302797902663,
          0.281481611958469,
          0.22112376630697952,
          0.1479609487565448,
          0.09222655388300591,
          0.32086085663876895,
          0.19432994573957427,
          0.25456295299176523,
          0.2891991309641192,
          0.24676547505354068,
          0.31336111354516905,
          0.37051720583356773,
          0.1275555403107364,
          0.2982326526988468,
          0.15210946493282645,
          0.22303546401100327,
          0.3146346585581361,
          0.1960044356961266,
          0.36870360880086916,
          0.12275466306311117,
          0.06834606547841401,
          0.25010093867638605,
          0.22409549784281502,
          0.16131187139375375,
          0.1448836571229428,
          0.17478709662965486,
          0.173133432210377,
          0.1738793093094504,
          0.15193150707734993,
          0.22304432024192256,
          0.20481542232876201,
          0.12813379186798352,
          0.1941576051278924,
          0.19122478723147143,
          0.18044632951757714,
          0.21690293284513926,
          0.1785848760125297,
          0.22242601551923696,
          0.3403232266957897,
          0.1130931516703555,
          0.13087466218942778,
          0.22444141901533812,
          0.3023297195769824,
          0.44931181978476337,
          0.16609622226059156,
          0.3061469764428647,
          0.25955834005289063,
          0.09336139135755922,
          0.10957396745441893,
          0.24912432363514897,
          0.2660656208268662,
          0.20086416356970052,
          0.27282041277377,
          0.19442534729517544,
          0.25245092297648686,
          0.38061613305975234,
          0.3644941554024427,
          0.16531600133668883,
          0.3132248735119704,
          0.0897178856179317,
          0.32687849192975044,
          0.12191390872820435,
          0.22808580877966375,
          0.3556616996257568,
          0.4583472892373617,
          0.24975981350271148,
          0.1427613966458713,
          0.21696220760319926,
          0.32101281848256386,
          0.4062964713216971,
          0.28654788065945946,
          0.24617829030632044,
          0.15766954556648266,
          0.15396226264092067,
          0.07649241093728436,
          0.1790265646469492,
          0.22819638754326171,
          0.17676157124289427,
          0.2621783974535719,
          0.21938458982033757,
          0.3573522742236963,
          0.16059899658099008,
          0.12113151264676911,
          0.4179349624815646,
          0.1795312335964668,
          0.23686385789269176,
          0.19961141215431097,
          0.2579155776730346,
          0.21271348482039754,
          0.13752106841708073,
          0.2640638421422939,
          0.26707212303772826,
          0.1938066101304678,
          0.17189868097670857,
          0.2574627773132879,
          0.26737162358451,
          0.24833649504441643,
          0.17739800873459538,
          0.19332883092398412,
          0.23853350303399745,
          0.2301169776162915,
          0.0982649967409149,
          0.2130180245302432,
          0.3504593674000135,
          0.12971599091926242,
          0.28170238087928035,
          0.22454182466383726,
          0.3781882002300857,
          0.25511862706469013,
          0.1334422013023104,
          0.3630341039794458,
          0.30206629775410254,
          0.29467546186314575,
          0.30813743420421363,
          0.16917371157677316,
          0.18552558455874207,
          0.21131142538887382,
          0.11668519655710058,
          0.16270303338307782,
          0.3926238345683497,
          0.17484393597383943,
          0.2468953699224205,
          0.15981349138107642,
          0.19346106235839416,
          0.1342386750310282,
          0.33948858907943946,
          0.3095537846562074,
          0.21152350041531226,
          0.16497059955796947,
          0.18448283336197957,
          0.18190678438070707,
          0.21979262122288756,
          0.12910493957731511,
          0.07677895066929234,
          0.4080060097465944,
          0.17683014451296514,
          0.12439095278603401,
          0.2674875885976924,
          0.12278505749852395,
          0.30469824087526765,
          0.16996041957801072,
          0.1329703279831954,
          0.18729583826860977,
          0.247995900494148,
          0.30892605159565734,
          0.23629440849676148,
          0.13825475375389928,
          0.09011624701757143,
          0.07244477436563498,
          0.21800204866058892,
          0.22344655803048552,
          0.3638217758260342,
          0.1475345281062925,
          0.19873595267224278,
          0.2561092649129174,
          0.22261111108166962,
          0.23726938711750428,
          0.11952825064022242,
          0.19060444178397112,
          0.21684406144385446,
          0.12185084820111576,
          0.2862128803103833,
          0.28218510216426806,
          0.15322641541231916,
          0.24517379183719085,
          0.11219255273668216,
          0.13216681335966368,
          0.139802818636726,
          0.305318055751025,
          0.20969971409690624,
          0.3623493322595563,
          0.09423583371255316,
          0.1722493125792483,
          0.2132763128247262,
          0.09962968588176042,
          0.21194661773453458,
          0.1361757394091839,
          0.469837719174368,
          0.13579025792268318,
          0.14802008228813746,
          0.28251941420909743,
          0.11039698809547097,
          0.16065404254981386,
          0.19804899122927688,
          0.2786406884046849,
          0.2359416835343211,
          0.2502824472137968,
          0.17202585320929753,
          0.23822234334879394,
          0.18577498877710774,
          0.2500518011600125,
          0.17337489257347352,
          0.24347586753413816,
          0.22308069393016644,
          0.2029060839412335,
          0.3537395028959701,
          0.21618448293204662,
          0.1647042214834964,
          0.14373929297203408,
          0.11662920836159302,
          0.18771338506379187,
          0.08178933811560439,
          0.1890243861910874,
          0.21968010873585098,
          0.17721915792666285,
          0.06334609229702505,
          0.21222049406086763,
          0.24391818180979474,
          0.2061975075436053,
          0.3045692355696503,
          0.3566441811242496,
          0.20576211918821366,
          0.24317675733985122,
          0.2755666030035024,
          0.08108947982740959,
          0.14727759413096747,
          0.18630811382091733,
          0.19188987954993048,
          0.18172487965349884,
          0.20527326472809293,
          0.25665809219327235,
          0.2897159047582832,
          0.1277312351451386,
          0.19645213752806387,
          0.2495872165448289,
          0.27028718979399247,
          0.1576374727543744,
          0.2387420588253426,
          0.2518271781027157,
          0.2884963055911075,
          0.35300495877619764,
          0.3589305176788877,
          0.3301186162232209,
          0.35095256337341607,
          0.1965190945104275,
          0.20337383482136412,
          0.31536677323045004,
          0.19073654247735716,
          0.2525271327442577,
          0.16470570945133772,
          0.25697454562738015,
          0.10432996234794159,
          0.2364058386561787,
          0.2812280956582528,
          0.4343522727071216,
          0.27912291199014827,
          0.23505774403285096,
          0.43107151631980917,
          0.17421271311356298,
          0.37162741552626366,
          0.20960624200679026,
          0.12346299401777847,
          0.31753587897896757,
          0.23427540889073356,
          0.15821314363795494,
          0.22399488274390197,
          0.26737606335413205,
          0.14577513445301493,
          0.1771442657700686,
          0.3505482205280241,
          0.20119926829924972,
          0.3046356242734342,
          0.26977807107128576,
          0.2994268086378906,
          0.2576738278908132,
          0.37469869539718054,
          0.29092802565655546,
          0.21489600088630717,
          0.1544551641529836,
          0.16760373336230827,
          0.19792700894409196,
          0.15702795408825718,
          0.1661773111197998,
          0.3361768092548396,
          0.20871843402440224,
          0.21611180352657838,
          0.07527686584996715,
          0.22918360725118792,
          0.4463268547491941,
          0.14709654032792974,
          0.06415362330074985,
          0.14994023642547566,
          0.2563029312012093
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "title": {
          "text": "variable"
         },
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Mini Batch Gradient Descent Cross Entropy Loss"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "iter"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "value"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "px.line(pd.DataFrame({\"loss\": m.losses}).rename_axis(\"iter\"), title=\"Mini Batch Gradient Descent Cross Entropy Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "1. [Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)\n",
    "2. [Logistics Regression, Wikipedia](https://en.wikipedia.org/wiki/Logistic_regression#Fit)\n",
    "3. [LaTex math symbols](https://www.math.uci.edu/~xiangwen/pdf/LaTeX-Math-Symbols.pdf)\n",
    "4. [Logistic Regression implementation by Ethen Liu](https://nbviewer.org/github/ethen8181/machine-learning/blob/master/text_classification/logistic.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
